{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aa7474",
   "metadata": {},
   "source": [
    "Now that we have our trained models we can make some predictions and evaluate their peformance. There is no need to train our model's on the training data again since RandomizedSearchCV refits on the best estimator by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5df9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import sys\n",
    "sys.path.append('..') #get root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76269996",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model = joblib.load('../models/svr_model.joblib')\n",
    "rfr_model = joblib.load('../models/rfr_model.joblib')\n",
    "dummy_model = joblib.load('../models/dummy_regressor.joblib')\n",
    "\n",
    "with open('../models/svr_grid_model_score.json', 'r') as file:\n",
    "    svr_log_score = json.load(file)\n",
    "    \n",
    "with open('../models/rfr_grid_model_score.json', 'r') as file:\n",
    "    rfr_log_score = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00f21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"../data/interim/X_test.csv\", index_col= 0) # expects an untransformed X_train\n",
    "y_test = pd.read_csv(\"../data/interim/y_test.csv\", index_col = 0)\n",
    "y_test_transformed = pd.read_csv(\"../data/processed/y_test_transformed.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782e81c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy predictions RSME in dollars: $386,309.06\n",
      "Dummy log predictions RSME: 0.5289\n",
      "Dummy predictions MAE in dollars: $225,367.89\n"
     ]
    }
   ],
   "source": [
    "dummy_predictions_log = dummy_model.predict(X_test) # gives a prediction in log space\n",
    "dummy_predictions = np.exp(dummy_predictions_log) \n",
    "\n",
    "dummy_predictions_rsme = root_mean_squared_error(y_test, dummy_predictions)\n",
    "dummy_log_predictions_rsme = root_mean_squared_error(y_test_transformed, dummy_predictions_log)\n",
    "dummy_predictions_mae = mean_absolute_error(y_test, dummy_predictions)\n",
    "dummy_log_predictions_mae = mean_absolute_error(y_test_transformed, dummy_predictions_log)\n",
    "\n",
    "\n",
    "print(f\"Dummy predictions RSME in dollars: ${dummy_predictions_rsme:,.2f}\") \n",
    "print(f\"Dummy log predictions RSME: {dummy_log_predictions_rsme:.4f}\")\n",
    "print(f\"Dummy predictions MAE in dollars: ${dummy_predictions_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf3e82",
   "metadata": {},
   "source": [
    "Store the scoring values from the final models and compare to the log_prediction_rsme to see if model is underfitting or overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d027b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR predictions RSME in dollars: $201,325.85\n",
      "SVR log predictions RSME: 0.2669\n",
      "SVR Grid Search RSME: 0.2902\n",
      "SVR predictions MAE in dollars: $107,141.97\n"
     ]
    }
   ],
   "source": [
    "svr_predictions_log = svr_model.predict(X_test) # gives a prediction in log space\n",
    "svr_predictions = np.exp(svr_predictions_log) \n",
    "\n",
    "svr_predictions_rsme = root_mean_squared_error(y_test, svr_predictions)\n",
    "svr_log_predictions_rsme = root_mean_squared_error(y_test_transformed, svr_predictions_log)\n",
    "svr_predictions_mae = mean_absolute_error(y_test, svr_predictions)\n",
    "svr_log_predictions_mae = mean_absolute_error(y_test_transformed, svr_predictions_log)\n",
    "\n",
    "print(f\"SVR predictions RSME in dollars: ${svr_predictions_rsme:,.2f}\")\n",
    "print(f\"SVR log predictions RSME: {svr_log_predictions_rsme:.4f}\")\n",
    "print(f\"SVR Grid Search RSME: {-svr_log_score:.4f}\")\n",
    "print(f\"SVR predictions MAE in dollars: ${svr_predictions_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0fa975b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR predictions RSME in dollars: $240,284.93\n",
      "RFR log predictions RSME: 0.2774\n",
      "RFR Grid Search RSME: 0.3001\n",
      "RFR predictions MAE in dollars: $118,114.06\n"
     ]
    }
   ],
   "source": [
    "rfr_predictions_log = rfr_model.predict(X_test)\n",
    "rfr_predictions = np.exp(rfr_predictions_log)\n",
    "\n",
    "rfr_predictions_rsme = root_mean_squared_error(y_test, rfr_predictions)\n",
    "rfr_predictions_log_rsme = root_mean_squared_error(y_test_transformed, rfr_predictions_log)\n",
    "rfr_predictions_mae = mean_absolute_error(y_test, rfr_predictions)\n",
    "rfr_log_predictions_mae = mean_absolute_error(y_test_transformed, rfr_predictions_log)\n",
    "\n",
    "print(f\"RFR predictions RSME in dollars: ${rfr_predictions_rsme:,.2f}\")\n",
    "print(f\"RFR log predictions RSME: {rfr_predictions_log_rsme:.4f}\")\n",
    "print(f\"RFR Grid Search RSME: {-rfr_log_score:.4f}\")\n",
    "print(f\"RFR predictions MAE in dollars: ${rfr_predictions_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd03a0",
   "metadata": {},
   "source": [
    "Both of the models have a lower RSME compared to the dummy regressor so the models aren't peforming horribly. The support vector regressor model beats the random forest regressor model in terms of having the lowest RSME and MAE. Our RSME tells us that on our predictions can deviate up to roughly $200k with more weight towards larger mistakes. The MAE tells us that on average our predictions are off by roughly $107k which compared to the mean of approximately $560k is about a 19% average error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
